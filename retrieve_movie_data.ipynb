{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/excellentwork/data-sourcing-challenge/blob/main/retrieve_movie_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMbfs1foW4Q9"
      },
      "source": [
        "### Import Required Libraries and Set Up Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "0YHVJGSoW4Q_",
        "outputId": "b7c382c9-b481-4801-df6c-70b9b806f9e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c302ffd7c0c8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Dependencies\n",
        "import requests\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuukYUqXW4Q_"
      },
      "outputs": [],
      "source": [
        "# Set environment variables from the .env in the local environment\n",
        "load_dotenv()\n",
        "\n",
        "nyt_api_key = os.getenv(\"NYT_API_KEY\")\n",
        "tmdb_api_key = os.getenv(\"TMDB_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyb51sytW4RA"
      },
      "source": [
        "### Access the New York Times API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zfuYan9W4RA"
      },
      "outputs": [],
      "source": [
        "# Set the base URL\n",
        "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
        "\n",
        "# Filter for movie reviews with \"love\" in the headline\n",
        "# section_name should be \"Movies\"\n",
        "# type_of_material should be \"Review\"\n",
        "filter_query = 'section_name:\"Movies\" AND type_of_material:\"Review\" AND headline:\"love\"'\n",
        "\n",
        "# Use a sort filter, sort by newest\n",
        "sort = \"newest\"\n",
        "\n",
        "# Select the following fields to return:\n",
        "# headline, web_url, snippet, source, keywords, pub_date, byline, word_count\n",
        "field_list = \"headline,web_url,snippet,source,keywords,pub_date,byline,word_count\"\n",
        "\n",
        "# Search for reviews published between a begin and end date\n",
        "begin_date = \"20130101\"\n",
        "end_date = \"20230531\"\n",
        "\n",
        "# Build URL\n",
        "params = {\n",
        "  'api-key': nyt_api_key,\n",
        "  'fq': filter_query,\n",
        "  'sort': sort,\n",
        "  'fl': field_list,\n",
        "  'begin_date': begin_date,\n",
        "  'end_date': end_date,\n",
        "  }\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3nFdUIVW4RA"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to store the reviews\n",
        "reviews_list = []\n",
        "\n",
        "# loop through pages 0-19\n",
        "for page in range(20):\n",
        "    # create query with a page number\n",
        "    # API results show 10 articles at a time\n",
        "    query_params = {\n",
        "        'api-key': userdata.get('NYT_API_KEY'),\n",
        "        'fq': filter_query,\n",
        "        'sort': sort,\n",
        "        'fl': field_list,\n",
        "        'begin_date': begin_date,\n",
        "        'end_date': end_date,\n",
        "        'page': page\n",
        "    }\n",
        "\n",
        "    # Make a \"GET\" request and retrieve the JSON\n",
        "    response = requests.get(url, params=query_params)\n",
        "    reviews = response.json()\n",
        "\n",
        "    # Add a twelve second interval between queries to stay within API query limits\n",
        "    time.sleep(12)\n",
        "\n",
        "\n",
        "    #Try and save the reviews to the reviews_list\n",
        "    try:\n",
        "      if reviews[\"response\"][\"docs\"]:\n",
        "\n",
        "        # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
        "        for review in reviews[\"response\"][\"docs\"]:\n",
        "          reviews_list.append(review)\n",
        "\n",
        "        # Print the page that was just retrieved\n",
        "        print(f\"Checked page {page}\")\n",
        "    except:\n",
        "        # Print the page number that had no results then break from the loop\n",
        "        print(f\"No results on page {page}, stopping.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezchE4prW4RB"
      },
      "outputs": [],
      "source": [
        "# Preview the first 5 results in JSON format\n",
        "# Use json.dumps with argument indent=4 to format data\n",
        "reviews_list_strings = [json.dumps(movie, indent=4) for movie in reviews_list[:5]]\n",
        "\n",
        "# Print the results to see the JSON strings of the first 5 entries\n",
        "for movie_str in reviews_list_strings:\n",
        "    print(movie_str)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX1dwwNRW4RB"
      },
      "outputs": [],
      "source": [
        "# Convert reviews_list to a Pandas DataFrame using json_normalize()\n",
        "reviews_df = pd.json_normalize(reviews_list)\n",
        "\n",
        "# Print the DataFrame\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG9NYdiGW4RB"
      },
      "outputs": [],
      "source": [
        "# Extract the title from the \"headline.main\" column and\n",
        "# save it to a new column \"title\"\n",
        "# Title is between unicode characters \\u2018 and \\u2019.\n",
        "# End string should include \" Review\" to avoid cutting title early\n",
        "\n",
        "# Create a new column 'title' by extracting the title using a lambda function\n",
        "reviews_df['title'] = reviews_df['headline.main'].apply(\n",
        "    lambda st: st[st.find(\"\\u2018\")+1:st.rfind(\"\\u2019 Review\")]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Print the DataFrame with the new column\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h1g9HJAW4RB"
      },
      "outputs": [],
      "source": [
        "# Extract 'name' and 'value' from items in \"keywords\" column\n",
        "def extract_keywords(keyword_list):\n",
        "    extracted_keywords = \"\"\n",
        "    for item in keyword_list:\n",
        "        # Extract 'name' and 'value'\n",
        "        keyword = f\"{item['name']}: {item['value']};\"\n",
        "        # Append the keyword item to the extracted_keywords list\n",
        "        extracted_keywords += keyword\n",
        "    return extracted_keywords\n",
        "\n",
        "# Fix the \"keywords\" column by converting cells from a list to a string\n",
        "reviews_df['keywords'] = reviews_df['keywords'].apply(extract_keywords)\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hDlj-TsxW4RB"
      },
      "outputs": [],
      "source": [
        "# Create a list from the \"title\" column using to_list()\n",
        "# These titles will be used in the query for The Movie Database\n",
        "titles = reviews_df[\"title\"].to_list()\n",
        "titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoRntWh4W4RC"
      },
      "source": [
        "### Access The Movie Database API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY6q_ZojW4RC"
      },
      "outputs": [],
      "source": [
        "# Prepare The Movie Database query\n",
        "url = \"https://api.themoviedb.org/3/search/movie?query=\"\n",
        "tmdb_key_string = \"&api_key=\" + tmdb_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPW9roxSW4RC"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to store the results\n",
        "tmdb_movies_list = []\n",
        "\n",
        "# Create a request counter to sleep the requests after a multiple\n",
        "# of 50 requests\n",
        "request_counter = 1\n",
        "\n",
        "# Loop through the titles\n",
        "for title in titles:\n",
        "\n",
        "    # Check if we need to sleep before making a request\n",
        "    if request_counter % 50 == 0:\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Add 1 to the request counter\n",
        "    request_counter += 1\n",
        "\n",
        "    # Perform a \"GET\" request for The Movie Database\n",
        "    headers = {\n",
        "      \"accept\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {tmdb_access_token}\"\n",
        "    }\n",
        "\n",
        "    response_general_query = requests.get(\n",
        "      f\"https://api.themoviedb.org/3/search/movie?query={title}\",\n",
        "      headers=headers\n",
        "    )\n",
        "    # Include a try clause to search for the full movie details.\n",
        "    # Use the except clause to print out a statement if a movie\n",
        "    # is not found.\n",
        "\n",
        "    try:\n",
        "        # Get movie id\n",
        "        response_movies_id = response_general_query.json()[\"results\"][0][\"id\"]\n",
        "\n",
        "        # Make a request for a the full movie details\n",
        "        # Execute \"GET\" request with url\n",
        "        response_full_details = requests.get(\n",
        "          f\"https://api.themoviedb.org/3/movie/{response_movies_id}\",\n",
        "          headers=headers\n",
        "        )\n",
        "        # Extract the genre names into a list\n",
        "        genres = extract_genres(response_full_details.json()[\"genres\"])\n",
        "\n",
        "        # Extract the spoken_languages' English name into a list\n",
        "        spoken_languages = extract_english_name(response_full_details.json()[\"spoken_languages\"])\n",
        "\n",
        "        # Extract the production_countries' name into a list\n",
        "        production_countries = extract_production_country(response_full_details.json()[\"production_countries\"])\n",
        "\n",
        "        # Add the relevant data to a dictionary and\n",
        "        # append it to the tmdb_movies_list list\n",
        "        tmdb_movies_list.append({\n",
        "            \"title\": response_full_details.json()['title'],\n",
        "            \"original_title\": response_full_details.json()['original_title'],\n",
        "            \"budget\": response_full_details.json()[\"budget\"],\n",
        "            \"original_language\": response_full_details.json()[\"original_language\"],\n",
        "            \"homepage\":response_full_details.json()[\"homepage\"],\n",
        "            \"overview\": response_full_details.json()[\"overview\"],\n",
        "            \"popularity\": response_full_details.json()[\"popularity\"],\n",
        "            \"runtime\" : response_full_details.json()[\"runtime\"],\n",
        "            \"revenue\": response_full_details.json()[\"revenue\"],\n",
        "            \"release_date\": response_full_details.json()[\"release_date\"],\n",
        "            \"vote_average\": response_full_details.json()[\"vote_average\"],\n",
        "            \"vote_count\": response_full_details.json()[\"vote_count\"],\n",
        "            \"genres\": genres,\n",
        "            \"spoken_languages\": spoken_languages,\n",
        "            \"production_countries\": production_countries\n",
        "\n",
        "        })\n",
        "\n",
        "        # Print out the title that was found\n",
        "        print(f\"Found {title}\")\n",
        "    except IndexError:\n",
        "        print(f\"{title} not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeeotQlkW4RC"
      },
      "outputs": [],
      "source": [
        "# Preview the first 5 results in JSON format\n",
        "# Use json.dumps with argument indent=4 to format data\n",
        "tmdb_movies_list_strings = [json.dumps(movie, indent=4) for movie in tmdb_movies_list[:5]]\n",
        "\n",
        "# Print the results to see the JSON strings\n",
        "for movie_str in tmdb_movies_list_strings:\n",
        "    print(movie_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iW36hK4W4RC"
      },
      "outputs": [],
      "source": [
        "# Convert the results to a DataFrame\n",
        "tmdb_search_results_df = pd.json_normalize(tmdb_movies_list)\n",
        "tmdb_search_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yla770pCW4RD"
      },
      "source": [
        "### Merge and Clean the Data for Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q48ats0MW4RD"
      },
      "outputs": [],
      "source": [
        "# Merge the New York Times reviews and TMDB DataFrames on title\n",
        "merged_df = pd.merge(reviews_df, tmdb_search_results_df, on=\"title\", how=\"inner\")\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Per1PhW4RD"
      },
      "outputs": [],
      "source": [
        "# Remove list brackets and quotation marks on the columns containing lists\n",
        "# Create a list of the columns that need fixing\n",
        "columns_to_fix = [\"spoken_languages\", \"genres\", \"production_countries\"]\n",
        "\n",
        "# Loop through the list of columns to fix\n",
        "for column in columns_to_fix:\n",
        "    # Convert the column to type 'str'\n",
        "    merged_df[column] = merged_df[column].astype(str).replace(r\"[\\[\\]'']\", \"\", regex=True)\n",
        "\n",
        "\n",
        "# Display the fixed DataFrame\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E40SOC4GW4RD"
      },
      "outputs": [],
      "source": [
        "# Drop \"byline.person\" column\n",
        "merged_df.drop('byline.person', axis=1, inplace=True)\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVd7fmNdW4RD"
      },
      "outputs": [],
      "source": [
        "# Delete duplicate rows and reset index\n",
        "\n",
        "# Convert the 'web_url' column to a hashable type, such as string\n",
        "print(type(merged_df['web_url']))\n",
        "merged_df['web_url'] = merged_df['web_url'].astype(str)\n",
        "\n",
        "# Delete duplicate rows and reset index\n",
        "merged_df = merged_df.drop_duplicates(subset='web_url', keep='first').reset_index(drop=True)\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv64-bxNW4RD"
      },
      "outputs": [],
      "source": [
        "# Export data to CSV without the index\n",
        "os.makedirs('output')\n",
        "merged_df.to_csv('output/collected_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AwgxluxW4RD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}